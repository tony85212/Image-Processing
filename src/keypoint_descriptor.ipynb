{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keypoint(Patch) Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import glob\n",
    "import os\n",
    "import cv2\n",
    "import PIL\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import torch\n",
    "import torch.nn.init\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "from tqdm import tqdm\n",
    "from torch.autograd import Variable\n",
    "from copy import deepcopy, copy\n",
    "from config_profile import args\n",
    "from Utils import cv2_scale36, cv2_scale, np_reshape, np_reshape64\n",
    "from Utils import L2Norm, cv2_scale, np_reshape\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the deep network: DesNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the CNN3 architecture\n",
    "class DesNet(nn.Module):\n",
    "    \"\"\"DesdNet model definition\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DesNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(32, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=2, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(64, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=2,padding=1, bias = False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1, bias = False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Dropout(0.3),\n",
    "            nn.Conv2d(128, 128, kernel_size=8, bias = False),\n",
    "            nn.BatchNorm2d(128, affine=False),\n",
    "        )\n",
    "        self.features.apply(weights_init)\n",
    "        return\n",
    "    \n",
    "    def input_norm(self,x):\n",
    "        flat = x.view(x.size(0), -1)\n",
    "        mp = torch.mean(flat, dim=1)\n",
    "        sp = torch.std(flat, dim=1) + 1e-7\n",
    "        return (x - mp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(-1).expand_as(x)) / sp.detach().unsqueeze(-1).unsqueeze(-1).unsqueeze(1).expand_as(x)\n",
    "    \n",
    "    def forward(self, input):\n",
    "        x_features = self.features(self.input_norm(input))\n",
    "        x = x_features.view(x_features.size(0), -1)\n",
    "        return L2Norm()(x)\n",
    "\n",
    "    \n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d):\n",
    "        nn.init.orthogonal_(m.weight.data, gain=0.6)\n",
    "        try:\n",
    "            nn.init.constant(m.bias.data, 0.01)\n",
    "        except:\n",
    "            pass\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load network\n",
    "# from descriptor import DesNet\n",
    "model = DesNet()\n",
    "if args.cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load network weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_weight_path = \"checkpoint.pth\" # suppose you are using checkpoint_4.pth\n",
    "test_model = DesNet()\n",
    "if args.cuda:\n",
    "    test_model.cuda()\n",
    "trained_weight = torch.load(trained_weight_path)['state_dict']\n",
    "test_model.load_state_dict(trained_weight)\n",
    "test_model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load raw patch files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patches_dir = \"train_patches.pth\"\n",
    "patches = torch.load(patches_dir)\n",
    "print(patches.shape)\n",
    "patches = patches.view(-1, 1, 32, 32).cuda()\n",
    "print(patches.shape)\n",
    "\n",
    "\n",
    "patches_dir = \"test_patches.pth\"\n",
    "patches2 = torch.load(patches_dir)\n",
    "print(patches2.shape)\n",
    "patches2 = patches2.view(-1, 1, 32, 32).cuda()\n",
    "print(patches2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='get_deep_features_module_cell'></a>\n",
    "### Get deep features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1700, 128])\n",
      "torch.Size([100, 17, 128])\n",
      "torch.Size([850, 128])\n",
      "torch.Size([50, 17, 128])\n"
     ]
    }
   ],
   "source": [
    "features = test_model(patches)\n",
    "print(features.shape)\n",
    "features = features.view(100, 5, 128).cpu().data  #x = NUM , y = keypoint, z = size\n",
    "print(features.shape)\n",
    "\n",
    "features2 = test_model(patches2)\n",
    "print(features2.shape)\n",
    "features2 = features2.view(50, 5, 128).cpu().data  #x = NUM , y = keypoint, z = size\n",
    "print(features2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to file, with the name of features_CNN*.pth\n",
    "features_dir = \"train_features.pth\"\n",
    "torch.save(features, features_dir)\n",
    "\n",
    "features_dir = \"test_features.pth\"\n",
    "torch.save(features2, features_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 2.8929e-02, -3.4354e-02,  1.7163e-02,  1.2051e-01, -9.7296e-02,\n",
      "         5.8959e-02,  1.6133e-02, -8.1970e-02, -4.7363e-02, -9.5427e-03,\n",
      "         1.3860e-01, -2.4629e-02,  5.6087e-02, -3.4598e-02,  9.3037e-02,\n",
      "         1.7789e-01,  3.5172e-02, -1.2053e-01, -1.8533e-01,  5.4706e-02,\n",
      "         6.5915e-02, -3.8367e-02,  8.7116e-04,  1.5862e-01, -5.3844e-02,\n",
      "        -1.4471e-01,  1.0705e-02,  2.9205e-02, -9.9659e-02, -8.9973e-02,\n",
      "        -1.8374e-01,  4.3521e-02,  9.1172e-02,  5.1403e-02,  8.2082e-03,\n",
      "        -1.3303e-01,  7.8862e-02,  6.5711e-03, -9.8523e-02,  6.1728e-05,\n",
      "        -5.0307e-02,  3.5834e-03,  4.2761e-02,  4.8837e-03,  7.2475e-02,\n",
      "        -1.5026e-01,  3.3830e-02, -5.4504e-02, -1.0197e-01, -5.8610e-02,\n",
      "        -2.1646e-01,  9.8381e-02,  1.4467e-02,  2.1669e-02,  9.2268e-02,\n",
      "         1.6890e-02,  7.9713e-02, -3.3049e-02,  2.8121e-02, -4.8997e-02,\n",
      "        -1.9121e-02, -6.0857e-03, -1.1361e-01, -7.6787e-02,  5.2484e-03,\n",
      "         1.4322e-02,  9.3462e-02,  5.8809e-02, -5.5507e-02,  1.0582e-01,\n",
      "        -1.1539e-02, -4.2734e-02,  5.2874e-03, -1.3473e-02, -1.5101e-01,\n",
      "        -1.6656e-01,  5.2668e-02,  7.1362e-02,  3.3113e-02,  3.2540e-02,\n",
      "        -1.2390e-01, -5.7989e-02, -4.3189e-03, -4.8557e-02, -7.4348e-02,\n",
      "         3.2291e-02, -1.5263e-01, -1.3309e-01,  1.2575e-01, -8.1942e-02,\n",
      "        -1.5850e-02,  1.3934e-01, -4.6933e-02,  9.1784e-02, -6.8634e-02,\n",
      "        -4.7421e-02, -5.5796e-02,  8.8450e-03, -1.6438e-02,  2.0802e-01,\n",
      "         1.4692e-02, -1.3476e-02, -1.2894e-02,  5.5664e-02,  1.3053e-01,\n",
      "        -2.5617e-02,  4.6241e-02,  8.3233e-03, -1.3343e-01, -6.4371e-02,\n",
      "        -5.6724e-02, -5.2393e-02,  7.7836e-02, -6.7408e-02,  1.7529e-01,\n",
      "        -2.8244e-02,  2.1322e-01,  4.7242e-02, -1.5885e-01, -1.1289e-01,\n",
      "        -2.4495e-01, -5.1524e-02,  9.3263e-02, -1.3139e-01, -2.0718e-02,\n",
      "        -7.2901e-02, -6.4875e-02, -2.6282e-02])\n",
      "tensor([ 0.0607, -0.0459,  0.0774,  0.1889, -0.0655, -0.0412, -0.1308, -0.0861,\n",
      "         0.0734,  0.0267,  0.0689, -0.0795, -0.0269, -0.1045,  0.0666, -0.0386,\n",
      "        -0.0878,  0.0401,  0.0416,  0.0762,  0.0056, -0.0999, -0.0167,  0.1979,\n",
      "         0.0560, -0.0369, -0.0008,  0.0139, -0.1811, -0.0315,  0.0162, -0.1107,\n",
      "        -0.0749, -0.0239,  0.1443, -0.0833,  0.1264, -0.0628, -0.0390, -0.0454,\n",
      "        -0.1181,  0.0383,  0.0172, -0.0269,  0.1948, -0.1108,  0.0082, -0.1684,\n",
      "        -0.1063,  0.0732, -0.0391,  0.0739,  0.0867, -0.1037, -0.0004, -0.0101,\n",
      "         0.2298, -0.0660, -0.0677,  0.0689, -0.0386,  0.1735, -0.0098,  0.1366,\n",
      "         0.0045, -0.0906,  0.0956,  0.0738,  0.1445, -0.0305,  0.1290,  0.2176,\n",
      "        -0.0371,  0.0450,  0.0657,  0.0020, -0.0037,  0.0049, -0.0191,  0.0508,\n",
      "         0.0312,  0.1584,  0.1077,  0.0114, -0.0393,  0.0613,  0.0308,  0.0401,\n",
      "         0.0439, -0.1068, -0.1568, -0.0608,  0.0566, -0.0643, -0.1151, -0.0475,\n",
      "        -0.0704, -0.0324,  0.0697, -0.0966,  0.0447,  0.0942, -0.1716, -0.1002,\n",
      "        -0.0037,  0.0697,  0.1055,  0.0034,  0.0758,  0.0633,  0.1369,  0.0615,\n",
      "        -0.1025, -0.0883,  0.0404, -0.0124,  0.1196, -0.0848, -0.1321, -0.0777,\n",
      "         0.0157,  0.0406, -0.0691,  0.0810,  0.0081,  0.1641, -0.0321, -0.0639])\n"
     ]
    }
   ],
   "source": [
    "features = torch.load(\"train_features.pth\")\n",
    "print(features[0][2])\n",
    "fearutes2 = torch.load(\"test_features.pth\")\n",
    "print(features2[0][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
